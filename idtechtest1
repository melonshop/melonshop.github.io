1. 네이버 기사 크롤링 프로그램

#step1.라이브러리 불러오기

import  requests

from  bs4  import  BeautifulSoup  as  bs

import  telegram

import  schedule

import  time



# step2.새로운 네이버 뉴스 기사 링크를 받아오는 함수



def get_new_links(query, old_links=[]):



    # (주의) 네이버에서 키워드 검색 - 뉴스 탭 클릭 - 최신순 클릭 상태의 url

    url = f'https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3Aall&is_sug_officeid=0'



    # html 문서 받아서 파싱(parsing)

    response = requests.get(url)

    soup = bs(response.text, 'html.parser')



    # 해당 페이지의 뉴스기사 링크가 포함된 html 요소 추출

    news_titles = soup.select('a.news_tit')



    # 요소에서 링크만 추출해서 리스트로 저장

    list_links = [i.attrs['href'] for i in news_titles]



    # 기존의 링크와 신규 링크를 비교해서 새로운 링크만 저장

    new_links = [link for link in list_links if link not in old_links]



    return new_links





# step3.새로운 네이버 뉴스 기사가 있을 때 텔레그램으로 전송하는 함수

def send_links(query):

    # 함수 내에서 처리된 리스트를 함수 외부에서 참조하기 위함

    global old_links



    # 위에서 정의했던 함수 실행

    new_links = get_new_links(query, old_links)



    # 새로운 메시지가 있으면 링크 전송

    if new_links:

        bot.sendMessage(chat_id=chat_id, text='방금 업데이트 된 ' + f"{query} 주제의 크롤링입니다.")

        for link in new_links:

            bot.sendMessage(chat_id=chat_id, text=link)



    # 없으면 패스

    else:

        pass



    # 기존 링크를 계속 축적하기 위함



    old_links += new_links.copy()





# 실제 프로그램 구동

if __name__ == '__main__':



    # 토큰을 변수에 저장

    bot_token = '본인텔레그램 봇의 토큰'

    bot = telegram.Bot(token=bot_token)



    # 가장 최근에 온 메세지의 정보 중, chat id만 가져옴 (이 chat id는 사용자(나)의 계정 id임)

    chat_id = bot.getUpdates()[-1].message.chat.id





    # #step4.검색할 키워드 설정

    # query  =  input('크롤링 할 뉴스기사 키워드를 입력하세요: ')

    queries = ["삼성카드","신한카드","현대카드","국민카드","신용카드","금융감독원"]



    for query in queries:



        # 위에서 얻은 chat id로 bot이 메세지를 보냄.

        bot.sendMessage(chat_id=chat_id,

                        text=f"{query}를 주제로 뉴스 기사 크롤링이 시작 되었습니다")



        # step5.기존에 보냈던 링크를 담아둘 리스트 만들기

        old_links = []



        # 주기적 실행과 관련된 코드 (hours는 시, minutes는 분, seconds는 초)

        job = schedule.every(10).seconds.do(send_links, query)



    while True:

        schedule.run_pending()

        time.sleep(1)



2. Action 스크립트

# Actions -> Python application

# This workflow will install Python dependencies, run tests and lint with a single version of Python

# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

# https://crontab.guru/

# 한국 11시면 cron에 11-9=2로 표현

# 한국 22시면 22-9 = 13

# 5분이라 걸어놓음 10분 후 쯤 15분에 알람 옴

# cron순서 - 분 시



name: Python application



on:

  schedule:

    - cron : 0 * * * *

#  push:

#    branches: [ main ]

#  pull_request:

#    branches: [ main ]



jobs:

  build:



    runs-on: ubuntu-latest



    steps:

      - uses: actions/checkout@v2

      - name: Set up Python 3.11.1

        uses: actions/setup-python@v2

        with:

          python-version: 3.11.1

      - name: Install dependencies

        run: |

          python -m pip install --upgrade pip

          pip install requests==2.27.1 bs4==0.0.1 python-telegram-bot==13.14 schedule

      - name: navercrawlingbot

        run: |

          python navernews.py
